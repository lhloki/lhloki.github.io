---
layout: post
title:  "classifier"
author: Hai Lan
date:   2018-02-14
categories: data
tags: [AI,R]
use_math: true
comments: false
---

分类器，与回归之间有着类似。最根本的，它们都在试图建立解释变量和响应变量之间的对应关系。这种关系，如果是线性的，则称之为线性回归和线性分类器。线性关系做适当的拓展，比如在解释变量端，加入交叉项目，对数项目等，都还可以用最小二乘的方法加以解决。另外的拓展，比如加在响应变量端，则往往称之为广义线性方法。最大似然估计是常见的解法。这是相似处。

分类器与回归最大的不同，在于回归模型的响应变量的数值之间是存在“序”的关系，比如2小于3。 而分类的响应变量虽然在计算系统中，我们也给予分配对应的数值，但是本质上，它们是factor，类别1与类别2之间不存在“序”的关系。这直接决定了，不管响应变量与解释变量之间的数学关系多么简单，我们也不能简单的套用回归的方法。

# 二类分类

对概率建模。$p(x)=Pr(y=1|X=x)$，最简单的模型
$$
p(x)=\beta_0+\beta^t x
$$
不合用。（显然p(x)有小于0或者大于1的可能）

则对$p(x)$的变形建模。
$$
\log(\frac{p(x)}{1-p(x)}) = \beta_0 + \beta^t x
$$

这就是所谓的logistic回归。最大似然估计是求解$\beta_0$和$\beta$的常用方法。在R中，使用glm(...,family='binomial')来实现logistic回归。

# 多类分类
linear discrimination analysis（线性鉴别分析）是常用的方法。作为有监督的学习方法，我们可以明确的估计$f_k(x)=P(X=x|Y=k)$，即类别k下，所有解释变量的概率（密度）分布。通常意义上，我们可以假设$f_k(X)$是服从高斯分布的，从而只需要估计均值与方差。从而，我们关心的观测x属于类别k的概率可以由贝叶斯理论写出：
$$
P(Y=k|X=x) =\frac{\pi_k f_k(x)}{\sum_{i=1}^{K}\pi_i f_i(x)}
$$
其中$\pi_i$为类别i出现的概率。
