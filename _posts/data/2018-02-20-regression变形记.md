---
layout: post
title:  "Regression变形记"
author: Hai Lan
date:  2018-02-20
categories: data
tags: [AI,R]
use_math: true
comments: false
---

我们集中讨论regression的一些技术问题。包括：

* 解释变量选择（全枚举法、前向法、后向法、混合法等）
* Ridge regression和lasso
* 降低维度方法

# 解释变量选择

在我们可以观察到的集合X中，可能包括了比较大的、一些无关紧要的维度，在建立模型的时候把它们统统都考虑进来，固然可以提高$R^2$，但是得到的模型也许并没有合适的解释能力与预测能力。毕竟$R^2$是针对训练集合而言的。而我们关心的，要么是解释变量与响应变量之间的逻辑联系，要么是模型在验证集，甚至在线的数据预测当中表现。因此选择适当的X的子集作为解释变量是必要的。如果$X$的维度为$p$，那么其所有可能的子集数目为$2^p$。在p的数目较小，不超过40个的时候，计算能力使得我们可以穷举所有可能的子集，在统一的标准，比如AIC、BIC或者交叉验证法之下，选择最优的X的子集。

然而当p较大的时候，计算上的难度，使得以上方法不能实际进行。变通的方法是，我们从启发性的检验一些比较可能包括最优子集的集合，从中选出相对最优的集合。比如前向法、后向法。

## 前向法
从空集开始，每次增加对于增加$R^2$最为有利的那个解释变量，直至X被考虑进来。然后在这$1+p$个模型中，按照选定的标准寻找最优模型。

## 后向法
与前向法相反，我们从X出发，每次减少一个解释变量，该变量对于减低$R^2$是最微弱的，直至空集被考虑进来。

以上两种方法的缺点是明显的。比如有$X_1,X_2,X_3$三个解释变量。一开始$X_1$被选中，接着$X_1,X_2$被选中。而实际上，如果只有两个解释变量的话，可能$X_2,X_3$是更好的组合，但是在$X_1$被选中的情况下，前向法是不会考虑$X_2,X_3$这样的组合的。

## 混合法
在前向法增加了一个变量后，考虑淘汰一个变量，同时避免重复，避免$R^2$低于前值。

# Ridge regression和lasso

我们知道线性回归中的最小二乘法，是最优无偏估计。在$n>>p$的情况下，模型的variance也不大，所以最小二乘是不二之选。但是当$p$较大，甚至接近n的时候。模型的variance就比较大了，这个时候坚持无偏估计往往不是一个好的选择。有偏估计，若能大幅度降低模型的variance则不失为好的选择。在这种情况下，往往引入shrinkage的方法：将估计的参数往群体均值（这里为0）收缩。这看起来可能矛盾，因为所谓的群体均值，可能是身高和学历这样不搭界的变量的均值。但是，统计理论告诉我们，它确实有效。

## Ridge regression

对应的优化目标是
$$
RSS + \lambda \sum_{i=1}^p \beta_i^2
$$

该方法把所有的$\beta_i$除开$\beta_0$往0压缩。而且是以一种“等比例”的方式进行的。

## Lasso

对应的优化目标是
$$
RSS + \lambda \sum_{i=1}^p |\beta_i|
$$
该方法也是把所有的$\beta_i$除开$\beta_0$往0压缩。但是是以一种“等幅度”的方式进行的。一些在压缩幅度范围内的$\beta_i$则直接变为0. 所以这样的shrinkage方法往往又被成为软阈值法。(soft thresholding)
