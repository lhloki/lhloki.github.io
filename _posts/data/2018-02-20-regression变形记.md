---
layout: post
title:  "Regression变形记"
author: Hai Lan
date:  2018-02-20
categories: data
tags: [AI,R]
use_math: true
comments: false
---

我们集中讨论regression的一些技术问题。包括：

* 解释变量选择（全枚举法、前向法、后向法、混合法等）
* Ridge regression和lasso
* 降低维度方法

# 解释变量选择

在我们可以观察到的集合X中，可能包括了比较大的、一些无关紧要的维度，在建立模型的时候把它们统统都考虑进来，固然可以提高$R^2$，但是得到的模型也许并没有合适的解释能力与预测能力。毕竟$R^2$是针对训练集合而言的。而我们关心的，要么是解释变量与响应变量之间的逻辑联系，要么是模型在验证集，甚至在线的数据预测当中表现。因此选择适当的X的子集作为解释变量是必要的。如果$X$的维度为$p$，那么其所有可能的子集数目为$2^p$。在p的数目较小，不超过40个的时候，计算能力使得我们可以穷举所有可能的子集，在统一的标准，比如AIC、BIC或者交叉验证法之下，选择最优的X的子集。

然而当p较大的时候，计算上的难度，使得以上方法不能实际进行。变通的方法是，我们从启发性的检验一些比较可能包括最优子集的集合，从中选出相对最优的集合。比如前向法、后向法。

## 前向法
从空集开始，每次增加对于增加$R^2$最为有利的那个解释变量，直至X被考虑进来。然后在这$1+p$个模型中，按照选定的标准寻找最优模型。

## 后向法
与前向法相反，我们从X出发，每次减少一个解释变量，该变量对于减低$R^2$是最微弱的，直至空集被考虑进来。

以上两种方法的缺点是明显的。比如有$X_1,X_2,X_3$三个解释变量。一开始$X_1$被选中，接着$X_1,X_2$被选中。而实际上，如果只有两个解释变量的话，可能$X_2,X_3$是更好的组合，但是在$X_1$被选中的情况下，前向法是不会考虑$X_2,X_3$这样的组合的。

## 混合法
在前向法增加了一个变量后，考虑淘汰一个变量，同时避免重复，避免$R^2$低于前值。

# Ridge regression和lasso

我们知道线性回归中的最小二乘法，是最优无偏估计。在$n>>p$的情况下，模型的variance也不大，所以最小二乘是不二之选。但是当$p$较大，甚至接近n的时候。模型的variance就比较大了，这个时候坚持无偏估计往往不是一个好的选择。有偏估计，若能大幅度降低模型的variance则不失为好的选择。在这种情况下，往往引入shrinkage的方法：将估计的参数往群体均值（这里为0）收缩。这看起来可能矛盾，因为所谓的群体均值，可能是身高和学历这样不搭界的变量的均值。但是，统计理论告诉我们，它确实有效。

## Ridge regression

对应的优化目标是
$$
RSS + \lambda \sum_{i=1}^p \beta_i^2
$$

该方法把所有的$\beta_i$除开$\beta_0$往0压缩。而且是以一种“等比例”的方式进行的。

## Lasso

对应的优化目标是
$$
RSS + \lambda \sum_{i=1}^p |\beta_i|
$$
该方法也是把所有的$\beta_i$除开$\beta_0$往0压缩。但是是以一种“等幅度”的方式进行的。一些在压缩幅度范围内的$\beta_i$则直接变为0. 所以这样的shrinkage方法往往又被成为软阈值法。(soft thresholding)

至于以上两个方法中$\lambda$的选取，没有什么特别的，无外乎枚举后寻找最优。

当$p$比较大，或者说相对于$n$比较大，除了以上的变量选择、Ridge regression和Lasso之外，我们还可以做降维的工作。降低维度的方法有很多种，在此，我们介绍主成份分析法和部分最小二乘法。

# 主成份分析法（PCA）

我们不谈主成份分析的线性代数的背景，事实上，特征向量和特征根的计算并不复杂，但是其含义往往被忽略掉了。主成份，依次代表了最能体现数据X的特征的空间上的直线。比如X的第一主成份，实际上就是X的行向量空间中，到个点距离最短的那条线。（同时也是各个点在该直线上投影的方差最大，勾股定理）第二主成份，就是垂直于第一主成份（或者说，统计无关，相关系数为0）并且到个点距离最短的那条直线（依旧是在该条直线上投影的方差最大）。距离最短，表明损失掉的信息最少；同理，方差最大表明保留的信息（不同）最多。

很明显一个$n\times p$的观察集X，对应的主成份最多也是$p$个，但是由于主成份变化中，对于信息的突出表达能力，我们可以使用远远小于$p$的前$m$个主成份来替代X而同时不至于损失太多的信息。这就达到了降低维度的作用。

以上方法都要求将数据标准化。因为距离必然受到单位的影响。

# 部分最小二乘法（partial least square）

主成份分析法，是寻找X的空间中，能够极大程度代表X的直线（或者说方向）。但是代表X未见得就能跟响应变量Y之间建立最有效的逻辑。说白了，主成份法是一种非监督学习法。在Y本身存在的情况下，缺乏结果导向的思维。简单的说，部分最小二乘法，就是寻找那些既能够代表X，也能够区分Y的方向。

计算$Z_1$的时候，我们满足
$$
Z_1 = \sum_{j=1}^p \phi_{j,1}X_{j}
$$
其中
$$
\phi_{j,1}=\frac{cov(X_j,Y)}{var(X_j)}
$$
计算$Z_2$时，将$Y$替换为$Y$在$Z_1$回归之后的残差。
